Experiment,Timestamp,Algorithm,Timesteps,Learning Rate,Total Return (%),Sharpe Ratio,Max Drawdown (%),vs SPY (%),Training Time (min),Description
baseline_200k,2025-11-30 13:25,PPO,200000,0.0003,48.28,1.016,-26.15,-0.31,5.5,Baseline - initial PPO agent
longer_training_500k,2025-11-30 13:53,PPO,500000,0.0003,60.93,1.07,-30.3,12.34,13.6,Testing if 500k timesteps improves performance
lower_lr,2025-11-30 14:09,PPO,500000,0.0001,48.02,0.982,-27.7,-0.56,12.2,Lower learning rate for more stable training
a2c_500k,2025-11-30 14:26,A2C,500000,0.0003,54.84,1.018,-28.89,54.84,9.9,A2C algorithm - often more stable than PPO
ppo_high_entropy,2025-11-30 14:46,PPO,500000,0.0003,62.91,1.333,-23.06,14.32,12.0,Higher entropy to reduce over-concentration
ppo_1m_high_entropy,2025-11-30 15:13,PPO,1000000,0.0003,86.94,1.617,-21.62,38.35,23.9,1M timesteps with high entropy - best combo?
a2c_500k,2025-11-30 15:29,A2C,500000,0.0003,54.84,1.018,-28.89,6.25,10.0,A2C algorithm - often more stable than PPO
ensemble_3agents,2025-11-30 21:19,PPO Ensemble (3 agents),1500000,0.0003,54.1,0.932,-33.02,5.51,34.9,Ensemble of 3 PPO agents with high entropy
ensemble_5agents,2025-12-01 01:25,PPO Ensemble (5 agents),2500000,0.0003,48.53,0.828,-33.34,-0.06,57.6,Ensemble of 5 PPO agents with high entropy
best_seed_456_1m,2025-12-01 05:46,PPO,1000000,0.0003,66.62,1.406,-21.75,18.03,23.2,Best performing seed (456) with 1M timesteps and high entropy
